% ------------------------------------------------------------------
% Exemplo de introdução gerada por textos dummys a partir do
% lipsum
%-------------------------------------------------------------------


\chapter{Introdução}
\label{cap:intro} % faço a referência na bibliografia



Nos últimos 20 anos o volume e a variedade de dados produzidos e armazenados pela humanidade aumentou em algumas ordens de grandeza.  Imagens, áudio, registros de viagens, redes sociais, microdados administrativos, exames médicos, dados genômicos, a lista é vasta. Acompanhando esse movimento, principalmente na indústria de tecnologia, as aplicações de Aprendizado de Máquina (\textit{Machine Learning}) aumentaram proporcionalmente.

O campo certamente não nasceu nos corredores do Vale do Silício. As primeiras contribuições formais na área são muito anteriores à qualquer forma de indústria de computação moderna e vêm da psicologia da consciência dos anos 40-50. Trabalhos como \citeonline{mcculloch1943logical} e \citeonline{rosenblatt1958perceptron} introduziram as primeiras redes neurais. Ferramentas similares começam a ser abordadas por estatísticos, procurando performance preditiva, e cientistas da computação, procurando inteligência artificial e automatização de processos, nos anos 70 e 80. É aí que surgem os SVMs \cite{vapnik1974theory} e Árvores de Decisão \cite{breiman1984classification}, partes do cânone da área.

Por ser um campo altamente interdisciplinar que prosperou na indústria e cujas técnicas são aplicadas com grande variação do software que as implementam, do ponto de vista do econometrista acostumado com manuais altamente consistentes entre si há pouca sistematização e consenso. Existem referências importantes como \citeonline{friedman2001elements}, mas a uniformidade de aplicação dessas técnicas é muito menor do que as suas primas vindas da estatística clássica. Isso reflete uma cisma que \citeonline{breiman2001statistical} coloca em termos de 'duas culturas da modelagem estatística'. 

A primeira, que o autor chama de 'modelagem baseada nos dados' assume que os dados coletados são gerados a partir de um processo estocástico que pode ser estimado. As noções de sucesso de modelagem, nessa abordagem, são cumprir uma bateria de testes estatísticos e boas leituras em indicadores de qualidade de ajuste. A segunda, 'modelagem algoritmica', considera o processo estocástico verdadeiro a ser aproximado potencialmente complexo demais e aborda a questão com caixas-pretas a serem estimadas com processos de decisão que em teoria seriam aproximações de processos cognitivos: encontrar hiperplanos separadores, partições recursivas de conjuntos, agrupamento por proximidade, decomposição em componentes principais, compor afirmações verdadeiro-falso, agrupamento hierárquico, etc. Nessa concepção, a validade de um modelo é determinada pela sua capacidade preditiva. 

Uma caixa-preta, no entanto, talvez não seja de tanto interesse ao econometrista aplicado que está sim preocupado com alguma forma de inferência. Em particular, distinguir estatisticamente efeitos de tratamento de zero. Existem métodos de interpretação de modelos \cite{ribeiro2016model}, mas pouca sistematização, em particular dentro da literatura em economia aplicada e econometria. 

O tema é relevante em dois sentidos. Do ponto de vista do aprendizado de máquina, porque encaixa em uma agenda maior de pesquisa em \textit{interpretabilidade} de Machine Learning. O núcleo duro reduzido da disciplina abre espaço para uma série de más práticas disseminadas no uso dessas técnicas \cite{flach2019performance}. Avaliar efeitos marginais pode ser usado como uma forma de validação qualitativa também. Relações com sinais inversos ao esperado podem ser evidência de problemas na estimação, entendimento do problema ou preparação dos dados.

Há também, pelo mesmo motivo, dificuldade de comunicação de resultados de modelos e até mesmo responsabilização civil-criminal quanto às consequências de seu uso em ambiente de produção, sem supervisão humana \cite{lepri2018fair}. Interpretação de modelo, em particular antes de entrega para algum ambiente de produção em que seus resultados afetarão desde experiência de uso em aplicativos de jogos à possivelmente investigação criminal, é crucial. O economista se preocupa com interpretabilidade porque é, de certa maneira, a finalidade principal do trabalho aplicado de métodos quantitativos. Estimar efeitos marginais, (semi)elasticidades e grandezas similares representa a esmagadora maioria das aplicações de econometria, salvo raros estudos como \citeonline{edison2020text} que usam técnicas não-supervisionadas vindas de Linguística Computacional.

Sobre a organização desta monografia. No capítulo 2 apresento a construção de um modelo de floresta aleatória partindo de primeiros princípios. No capítulo 3 discuto a teoria clássica de regressão linear, suas hipóteses, virtudes e limitações, amparado por simulações de Monte Carlo. Bem como a identificação de efeitos marginais em modelos lineares e a problemática envolvida em computa-los em modelos de floresta aleatória. No capítulo 4 apresento um estudo de caso ilustrando estimação, validação e interpretação de modelos de floresta aleatória, bem como uma ilustração da técnica. Por fim, no capítulo 5, ressalto limitações e agendas de pesquisa a partir daqui.


