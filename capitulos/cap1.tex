% ------------------------------------------------------------------
% Exemplo de introdução gerada por textos dummys a partir do
% lipsum
%-------------------------------------------------------------------


\chapter{Introdução}
\label{cap:intro} % faço a referência na bibliografia



Nos últimos 20 anos o volume e a variedade de dados produzidos e armazenados pela humanidade aumentou em algumas ordens de magnitude.  Imagens, registros de viagens, redes sociais, microdados administrativos, exames médicos, dados genômicos, a lista é vasta. Acompanhando esse movimento, principalmente na indústria de tecnologia, as aplicações de Aprendizado de Máquina (\textit{Machine Learning}) aumentaram proporcionalmente.

O campo certamente não nasceu nos corredores do Vale do Silício. As primeiras contribuições formais na área são muito anteriores à qualquer forma de indústria de computação moderna e vêm da psicologia da consciência dos anos 40-50. Trabalhos como \citeonline{mcculloch1943logical} e \citeonline{rosenblatt1958perceptron} introduziram as primeiras redes neurais. Ferramentas similares começam a ser abordadas por estatísticos, procurando performance preditiva, e cientistas da computação, procurando inteligência artificial e automatização de processos, nos anos 70 e 80. É aí que surgem os SVMs \cite{vapnik1974theory} e Árvores de Decisão \cite{breiman1984classification}, partes do cânone da área.

Por ser um campo altamente interdisciplinar que prosperou na indústria e cujas técnicas são aplicadas com grande variação do software que as implementam, do ponto de vista do econometrista acostumado com manuais altamente consistentes entre si há pouca sistematização e consenso. Existem referências importantes como \citeonline{friedman2001elements}, mas a uniformidade de aplicação dessas técnicas é muito menor do que as suas primas vindas da estatística clássica. 

\citeonline{breiman2001statistical} se refere à "duas culturas da modelagem estatística". A primeira, que o autor chama de "modelagem baseada nos dados" assume que os dados coletados são gerados a partir de um processo estocástico que pode ser estimado. As noções de sucesso de modelagem, nessa abordagem, são cumprir uma bateria de testes estatísticos e boas leituras em indicadores de qualidade de ajuste. A segunda, "modelagem algoritmica", considera o processo estocástico verdadeiro a ser aproximado potencialmente complexo demais e aborda a questão com caixas-pretas a serem estimadas não com otimização mas com processos de decisão implementados por certos algoritmos: encontrar hiperplanos separadores, partições recursivas de conjuntos, agrupamento matemático, decomposição em componentes principais, compor afirmações verdadeiro-falso, agrupamento hierárquico, etc. Nessa concepção, a validade de um modelo é determinada pela sua capacidade preditiva. 

Uma caixa-preta, no entanto, talvez não seja de tanto interesse ao econometrista aplicado que está sim preocupado com alguma forma de inferência. Em particular, distinguir estatisticamente efeitos de tratamento de zero. Existem métodos de interpretação de modelos \cite{ribeiro2016model}, mas pouca sistematização, em particular dentro da literatura em economia. 

O método aqui ilustrado de computação de efeitos marginais gera um resultado de interpretação idêntico ao de uma regressão quantílica, uma ferramenta padrão no campo, sem o custo de precisão das estimativas envolvido em fatiar a amostra nos quantis e consiste em um procedimento computacionalmente simples, implementável em poucos comandos em qualquer linguagem de programação de alto nível moderna. 

